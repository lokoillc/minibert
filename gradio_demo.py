import torch
from transformers import BertTokenizer
from config import MiniBertConfig
from train.train import MiniBertForSequenceClassification
import gradio as gr

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1. åŠ è½½ Tokenizer & Config
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
config = MiniBertConfig(vocab_size=tokenizer.vocab_size)

# 2. åŠ è½½æ¨¡å‹
model = MiniBertForSequenceClassification(config, num_labels=2)
model.load_state_dict(torch.load("C:\\Users\\Administrator\\PycharmProjects\\minibert\\"
                                 "train\\checkpoints\\best_moe_model.pt", map_location=device))
model.to(device)
model.eval()


# 3. æ¨ç†å‡½æ•°
def predict(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs["logits"]
        predicted_class = logits.argmax(dim=-1).item()

    return "æ­£é¢ ğŸ˜Š" if predicted_class == 1 else "è´Ÿé¢ ğŸ˜"


# 4. Gradioç•Œé¢

gr.Interface(
    fn=predict,
    inputs=gr.Textbox(lines=2, placeholder="è¯·è¾“å…¥ä¸­æ–‡å¥å­ï¼Œä¾‹å¦‚ï¼šè¿™éƒ¨ç”µå½±çœŸå¥½çœ‹ï¼"),
    outputs=gr.Label(num_top_classes=2),
    title="MiniBERT ä¸­æ–‡æƒ…æ„Ÿåˆ†ç±» Demo",
    description="è¾“å…¥ä»»æ„ä¸­æ–‡å¥å­ï¼Œæ¨¡å‹å°†åˆ¤æ–­å…¶æƒ…æ„Ÿï¼ˆæ­£é¢/è´Ÿé¢ï¼‰ã€‚"
).launch()
